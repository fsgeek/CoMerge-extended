\subsection{Data Structure Sensitivity}
\label{sec:placement}


\begin{table*}[t]
\centering
{\footnotesize
	\begin{subtable}{0.31\linewidth}
    \begin{tabular}{|p{1cm}|p{0.8cm}|p{0.55cm}|p{0.55cm}|p{0.75cm}|} \hline
	FastMem alloc & Time & Benefit & Co-benefit & Size \\\hline \hline	
	All & 35.37 s & 1 & 3 & 1 GB\\\hline
	{\ttfamily nuclide} & 42.58 s & 0.9 & 2.7 & 30 MB \\\hline
	{\ttfamily energy} & 102.38 s & 0.14 & 0.42 & 970 MB \\\hline
      None & 113.17 s & 0 & 0 & 0 \\\hline
    \end{tabular}
	\subcaption{XSBench (high)}
	\label{tbl:tiering_xsbench}
	\end{subtable}
}
{\footnotesize
	\begin{subtable}{0.31\linewidth}
    \begin{tabular}{|p{1cm}|p{0.8cm}|p{0.55cm}|p{0.55cm}|p{0.85cm}|} \hline
	FastMem alloc & Time & Benefit & Co-benefit & Size \\\hline \hline	
	All & 49.3 s & 1 & 3 & 1.5 GB\\\hline
	{\ttfamily parts} & 160.1 s & 0.019 & 0.057 & 250 MB \\\hline
	{\ttfamily zones} & 49.4 s & 0.99 & 2.97 & 1250 MB \\\hline
      None & 162.2 s & 0 & 0 & 0 \\\hline
    \end{tabular}
	\subcaption{CLOMP (high)}
	\label{tbl:tiering_clomp}
	\end{subtable}
}
{\footnotesize
	\begin{subtable}{0.31\linewidth}
    \begin{tabular}{|p{1cm}|p{0.8cm}|p{0.55cm}|p{0.55cm}|p{0.75cm}|} \hline
	FastMem alloc & Time & Benefit & Co-benefit & Size \\\hline \hline	
	All & 28.83 s & 1 & 3 & 1.6 GB\\\hline
	{\ttfamily a} & 75.38 s & 0.4 & 1.2 & 534 MB \\\hline
	{\ttfamily b} & 75.53 s & 0.39 & 1.17 & 534 MB \\\hline
	{\ttfamily }c & 61.19 s & 0.59 & 1.77 & 534 MB \\\hline
      None & 106.55 s & 0 & 0 & 0 \\\hline
    \end{tabular}
	\subcaption{STREAM (high)}
	\label{tbl:tiering_stream}
	\end{subtable}
}

{\footnotesize
	\begin{subtable}{0.3\linewidth}
    \begin{tabular}{|p{1cm}|p{0.8cm}|p{0.55cm}|p{0.55cm}|p{0.75cm}|} \hline
	FastMem alloc & Time & Benefit & Co-benefit & Size \\\hline \hline
	All & 29.45 s &  1 & 0 & 1 GB \\\hline 
	{\ttfamily C4} & 29.68 s & 0.81 & 0 & 1 MB \\\hline
	{\ttfamily sum} & 29.9 s & 0.63 & 0 & 498 MB \\\hline
	{\ttfamily }A & 30.5 s & 0.15 & 0 & 498 MB \\\hline
	None & 30.69 s & 0 & 0 & 0 \\\hline
    \end{tabular}
	\subcaption{doitgen (none)}
	\label{tbl:tiering_doitgen}
	\end{subtable}
}
{\footnotesize
	\begin{subtable}{0.3\linewidth}
    \begin{tabular}{|p{1cm}|p{0.8cm}|p{0.55cm}|p{0.55cm}|p{0.75cm}|} \hline
	FastMem alloc & Time & Benefit & Co-benefit & Size \\\hline \hline	
	All & 116.86 s & 1 & 1 & 1.5GB \\\hline
	{\ttfamily X} & 136.38 s & 0.62 & 0.62 & 500 MB \\\hline
	{\ttfamily B} & 141.17 s & 0.53 & 0.53 & 500 MB \\\hline
	{\ttfamily A} & 150.47 s & 0.34 & 0.34 & 500 MB \\\hline
     None & 168.15 s & 0 & 0 & 0 \\\hline
    \end{tabular}
	\subcaption{adi (low)}
	\label{tbl:tiering_adi}
	\end{subtable}
}
{\footnotesize
	\begin{subtable}{0.3\linewidth}
    \begin{tabular}{|p{1cm}|p{0.8cm}|p{0.55cm}|p{0.55cm}|p{0.75cm}|} \hline
	FastMem alloc & Time & Benefit & Co-benefit & Size \\\hline \hline	
	All & 56.44 s & 1 & 2 & 250 MB \\\hline
	{\ttfamily B} & 56.98 s & 0.99 & 1.98 & 125 MB \\\hline
	{\ttfamily A} & 130.25 s & 0.04 & 0.08 & 125 MB \\\hline
      None & 133.05 s & 0 & 0 & 0 \\\hline
    \end{tabular}
	\subcaption{trmm (medium)}
	\label{tbl:tiering_trmm}
	\end{subtable}
}
{\footnotesize
	\begin{subtable}{0.33\linewidth}
    \begin{tabular}{|p{1cm}|p{0.8cm}|p{0.55cm}|p{0.55cm}|p{0.75cm}|} \hline
	FastMem alloc & Time & Benefit & Co-benefit & Size \\\hline \hline	
	All & 49.55 s & 1 & 3 & 1.5 GB \\\hline
	{\ttfamily hz} & 114.07 s & 0.48 & 1.44 & 500 MB \\\hline
	{\ttfamily ey} & 127.91 s & 0.37 & 1.11 & 500 MB \\\hline
	{\ttfamily ex} & 138.83 s & 0.29 & 0.87 & 500 MB \\\hline
      None & 174.58 s & 0 & 0 & 0 \\\hline
    \end{tabular}
	\subcaption{fdtd-2d (high)}
	\label{tbl:tiering_fdtd}
	\end{subtable}
}
{\footnotesize
	\begin{subtable}{0.33\linewidth}
    \begin{tabular}{|p{1cm}|p{0.8cm}|p{0.55cm}|p{0.55cm}|p{0.75cm}|} \hline
	FastMem alloc & Time & Benefit & Co-benefit & Size \\\hline \hline	
	All & 28.67 s & 1 & 3 & 1 GB\\\hline
	{\ttfamily A} & 59.64 s & 0.61 & 1.83 & 500 MB \\\hline
	{\ttfamily B} & 62.52 s & 0.58 & 1.74 & 500 MB \\\hline
      None & 109.7 s & 0 & 0 & 0 \\\hline
    \end{tabular}
	\subcaption{jacobi-2d (high)}
	\label{tbl:tiering_jacobi}
	\end{subtable}
}

\caption{Execution time in data tiering. Application sensitivity classification in parentheses.}
\label{tbl:tiering}
\vspace{-1ex}
\end{table*}

Next, we analyse the application sensitivity to data tiering across FastMem and SlowMem, using the testbed described in Section~\ref{sec:methodology}. 
We choose a few representative kernels from Polybench Suite across different sensitivity groups, as well as the three CORAL benchmarks, and look deeper into the algorithm that they implement, as well as the primary data structures they define. 
Then, we run each benchmark with different placement configurations. More
specifically, using the Linux NUMA API we place into FastMem one data structure at a time, allocating the rest in SlowMem, and measure the overall 
execution time of the application. 
This type of data tiering allows us to capture the benefit of 
placing a particular data object into FastMem, by observing the
difference in execution time between the two distinct configurations,
where all data is placed either in FastMem or SlowMem. 

We define the {\em benefit} of placing data object $O$ into FastMem as follows, where t is the corresponding application execution time, F and S are the execution times when all objects are in FastMem and SlowMem, respectively.
\[Benefit(O) = \frac{t(O)-S}{F-S}\]
The benefit factor can range between 0, which is the all-in-SlowMem configuration, to 1 being the all-in-FastMem configuration.
A high benefit factor close to 1, indicates that the data structure $O$
significantly contributes to the application's memory sensitivity, and
should get high priority in getting allocated in FastMem. Thus, using these benefit factor values for the different data objects of an
application, we are able to define a partial ordering of them, in a
similar way done by the related work described in Section~\ref{sec:intro}. Table~\ref{tbl:tiering} summarizes the execution times and benefit calculations across kernels  
from different sensitivity levels. 
More specifically, the first row of each table represents the best case scenario, where all data objects are allocated in FastMem, therefore the benefit factor is 1 and the size reflects the total 
memory footprint of the application. Similarly, the last row corresponds to the worst case scenario, where all data allocations are serviced from SlowMem, forcing a zero benefit factor. The middle rows 
highlight the execution time of the application, when only one particular object is allocated in FastMem, and the performance benefit the application gets, when this object is accessed through FastMem. 
The notion of co-benefit factor will be introduced in Section~\ref{subsec:merge}.\\

%\subsubsection
\vspace{0.6ex}
\noindent{\bf\em CORAL Experiments}
\vspace{0.3ex} 

\noindent The bechmarks from the CORAL suite are all very sensitive to execution over SlowMem, thus we do a deep-dive analysis for each one.\\

\noindent{\bf XSBench.} XSBench is mini app that simulates the  most computationally intensive part of the Monte Carlo transport algorithm, which is the calculation of macroscopic neutron cross-sections~\cite{Tramm:wy}. As per the official CORAL description, the purpose of XSBench is to stress the memory subsystem of a single node. The user can define the number of nuclides and gridpoints per nuclide, as well as the number of lookups of cross-section data that will be performed. The first two define the dataset size and the last one the length of the computation. The mini app consists of two discrete phases, the initialization of the data, which is done by the main thread, and the core computation of lookups, which is done in parallel by the number of thread the user configures. If the number of lookups defined is not big enough, then the initialization phase dominates the run time, as we observed in Section~\ref{sec:sensitivity}.  There are two main data structures, the {\ttfamily nuclide grid},
 which holds the matrix of nuclide grid points, and the {\ttfamily energy grid} which holds the corresponding energy values. 

We experiment with 12 threads, 355 nuclides, 2000 grid points per nuclide and 50 million lookups. Table~\ref{tbl:tiering_xsbench} holds the raw values of the total run time (initialization and computation phase), when all data is allocated in FastMem only and SlowMem only, as well as the benefit factors of the individual data structures. We observe that the data structure {\ttfamily energy grid} has only a benefit factor of 0.14 compared to the 0.9 of {\ttfamily nuclide grid}, even though its size is extremely larger, being 970 MB compared to 30 MB. This is due to the fact that accesses to the {\ttfamily nuclide grid} result in a significant number of Last Level Cache misses due to its access pattern, thus the mini app spends a lot of stall cycles waiting to load {\ttfamily nuclide grid} data from memory~\cite{Pena:2016}. 
This is why the application benefits immensely when this object is allocated in FastMem. 
Therefore, we not only see that there may be significant difference in the contribution of the different data structures to the overall application slowdown, but also that this difference may be independent of the size of the object itself. \\

\noindent{\bf CLOMP.} CLOMP is a benchmark designed to capture OpenMP overheads, as well as general threading, NUMA, caching, prefetching and memory latency and bandwidth factors that impact application performance~\cite{Bronevetsky:2008}. There are two main data structures that formulate an unstructured mesh, {\ttfamily parts} and {\ttfamily zones} which correspond to the two dimensions of the mesh.  The computation over the mesh is very basic algebra, so that the result can be verifiable. Essentially, {\ttfamily parts} is a list of pointers to the {\ttfamily zones}, and the computation is done over the {\ttfamily zones}. The benchmark consists of 7 individual sub-tests running one after the other, testing different individual threading overheads. 

We experiment with 12 threads and a mesh of 64000 parts of 640 zones each, where each zone is 32 bytes. 
Table~\ref{tbl:tiering_clomp} shows the overall slowdown of the aggregate execution time of all the 7 tests when all data is allocated in SlowMem, compared to the case where all fits in FastMem. We see that the {\ttfamily parts} list of pointers has trivial benefit factor, something that is expected because it is accessed only as a gateway to the {\ttfamily zones}.  {\ttfamily Zones} is the main data structure where computation is done as it is also reflected in its size. This is the reason why {\ttfamily zones} has benefit factor value almost close to 1. Thus, it is crucial that it is allocated in FastMem, otherwise the impact of using FastMem will be negligible and the application runtime will be as if all data was allocated in SlowMem.\\

\noindent{\bf STREAM.} STREAM is the predominant synthetic benchmark  to measure the bandwidth of a memory node~\cite{stream}. It reports the computation rate (MB/s) and execution time of four distinct kernels over three matrix data structures {\ttfamily a}, {\ttfamily b} and {\ttfamily c}. More specifically, the computation kernel is  \(c = a\) for `copy', \(b = scalar*c\) for `scale', \(c = a + b\) for `add', and \(a = b + scalar*c\) for `triad'.

We experiment with 12 threads and matrix size of 70 million elements each. Table~\ref{tbl:tiering_stream} shows the aggregate runtime of all four kernels when the matrices are allocated in FastMem and the slowdown when all are in SlowMem. The different matrices have similar benefit factor values, with matrix {\ttfamily c} having the higher one, due to the fact that it is accessed across all four kernels, whereas {\ttfamily a} and {\ttfamily b} participate only in three kernels. Therefore, {\ttfamily c} should be prioritized for allocation in FastMem, following {\ttfamily a} and then {\ttfamily b} who have trivial difference in the benefit factor value. \\

%\subsubsection
\vspace{1ex}
\noindent{\bf\em Polybench Experiments}
\vspace{0.3ex}

\noindent Overall, there is great variability in the benefit factor values of the different data objects amongst the various Polybench kernels. We look deeper into what are the algorithm and data structures of the following kernels, which belong in different sensitivity groups. \\

%\AG{make the way you're presenting this consistent with the above. I made a change in the first kernel. }

\noindent{\bf doitgen.} The multiresolution analysis kernel doitgen
uses two 3D matrices {\ttfamily A} and {\ttfamily sum} and one 2D matrix {\ttfamily C4}. The computation can be summarized as $sum = sum + A*C4$. 
Since the kernel is not sensitive to the presence of SlowMem, we see small variability in the execution time for the different tiering configurations, as per Table~\ref{tbl:tiering_doitgen}. 
However, the benefit calculations can still define a partial ordering of {\ttfamily C4 > sum > A}. 
Although {\ttfamily C4} is of trivial size compared to the other data structures, it receives such an amount of accesses that attributes a higher benefit factor than the others.\\

\noindent{\bf adi.} The alternating direction implicit solver kernel uses three two-dimensional matrices of the same size. The mathematical formula is more complex, but it is such that the number of accesses per matrix give a different benefit factor value to each one, ordering them as {\ttfamily X > B > A}, as per Table~\ref{tbl:tiering_adi}. However, as far as the total runtime is concerned, there is no significant difference across the various data tierings, because the kernel has low overall sensitivity to execution over slower memory.\\

\noindent{\bf trmm.} The triangular matrix multiply kernel calculates a triangular matrix multiplication of $B = A*B$, where {\ttfamily A} and {\ttfamily B} are matrices of the same size. The fact that matrix {\ttfamily B} is getting much more read and write accesses than {\ttfamily A}, in combination with 
the irregular access pattern of the triangular multiplication, results in the accesses to matrix {\ttfamily B} being crucial to the overall application performance, as we can see in Table~\ref{tbl:tiering_trmm}. \\

\noindent{\bf fdtd-2d.} The 2D finite different time domain kernel uses three two-dimensional matrices of the same size, which have similar benefit factor values. Overall, the application is very sensitive to slower memory, because every data tiering configuration incurs between 2x-3.5x slowdown from the all-in-FastMem configuration, as per Table~\ref{tbl:tiering_fdtd}, thus intelligent data placement of the different data structures is crucial to performance.\\

\noindent{\bf jacobi-2d.} The 2D jacobi stencil computation kernel uses two two-dimensional arrays {\ttfamily A} and {\ttfamily B} of equal size, in order to solve the $A*x = B$ using the Jacobi method. Both matrices are almost equally important for performance, as their benefit factor values are very similar, according to Table~\ref{tbl:tiering_jacobi}. Also, the kernel overall is very sensitive to slower memory and the benefit factors of the matrices are around 0.5 meaning that placing even one of them in FastMem can mitigate the overall slowdown by 50\%.\\

\noindent{\bf\em Takeaways}
\vspace{0.3ex}

\noindent Overall, we see that there can be high variability in the contribution of each data structure to the application runtime. We capture this contribution into a {\it benefit factor} value. On the one hand, there are kernels whose data objects have similar such values, so allocating any of them in FastMem can mitigate to some degree the slowdown from running the whole application over SlowMem. On the other side, there are kernels where the difference between the benefit factor of the objects is extremely significant and placement of one data structure to FastMem is the only way to mitigate the slowdown from allocating all data in SlowMem. Also, these observations apply to both single as well as multi-threaded applications, highlighting the importance for clever data placement decisions over a broad spectrum of application kernels and algebraic computations. Moving forward, these remarks are especially important to be captured in shared environments, where 
co-existing applications will compete for the available FastMem and sophisticated data tiering solutions are the only way to maximize performance across a group of applications, instead of just one. 

%Since I'm not presenting the Pin numbers, I don't want to mention them at all.
\begin{comment}
Using Pin we get the number of accesses that each data structure incurs. Metrics such as the size, the access pattern, the access frequency and density are used in related work, in order to 
attribute similar notions of cost on a data object granularity. 
In this way, we validate that the definition of cost, that the related
work provides, is correct and indeed reflects on the actual overall
execution time of the application. For brevity we present the
execution times and benefit calculations and not the raw numbers
related to accesses obtained with Pin. 
\end{comment}

%Following is a deep dive analysis of six representative kernels from Polybench/C. 




\begin{comment}

%\paragraph
\noindent{\bf doitgen:} Multiresolution analysis kernel. Non sensitive.\\
The kernel uses two 3D matrices $A$ and $sum$ and one 2D matrix $C4$. The computation can be summarized as $sum = sum + A*C4$. 
Since the kernel is not sensitive to the presence of SlowMem, we see small variability in the execution time for the different tiering configurations. 
However, the benefit calculations can still define a partial ordering of $C4 > sum > A$. This agrees with cost calculations that take into account the access frequency. 
Although $C4$ takes up only 512 KB, it receives a significantly high amount of read requests compared to $sum$, which receives few writes and $A$ which incurs few reads, when each take up 128 MB. 
Thus, both the benefit and cost computations imply that $C4$ should have
higher priority, than $sum$ and $A$, into being allocated in FastMem. 

However, it is important to observe that, in the 
case of non-sensitive kernels, there is no real performance benefit in performing data tiering. Rather, it imposes extra overhead in tracking the required metrics on a 
data structure granularity. Instead, if a kernel is classified as non sensitive the best practice would be to allocate all data in SlowMem, so as to eliminate extra calculations and 
preserve free space in FastMem for other possible co-located sensitive applications.%\\

%\noindent
%\paragraph
{\bf trmm:} Triangular matrix multiply. Latency sensitive.\\
The kernel multiplies the 2D matrices $A$ and $B$ according to the formula $B = alpha*A*B$, where $alpha$ is a constant.
The benefit calculations indicate that there is 0.99 benefit from placing $B$ in FastMem, whereas $A$ incurs only 0.04, defining a priority order of $B>A$. 
This also agrees with cost calculations, which 
take into account the fact that $A$ and $B$ have the same size, same number of reads, but $B$ also incurs writes as it stores the multiplication result. 
The big difference in the benefit value suggests that $A$ can be allocated in SlowMem, not impacting performance, and reducing in half the space needed in FastMem. %\\

%\noindent
%\paragraph
{\bf bicg:} Biconjugate gradient stabilized method, portion of BiCGStab linear solver. Latency sensitive.\\
The kernel uses a 2D matrix $A$ and 1D matrices $p$, $q$, $r$, $s$ to compute $s =s + r*A$ and $q =q + A*p$. 
The benefit values form a partial ordering of $A > s = q > p = r$. This can be justified by the fact that $A$ is significantly big in size (3 GB) and receives most reads and writes, 
whereas the rest data objects are only 156 KB each, receive the same amount of reads, whereas $q$ and $s$ incur additional writes as well.
Therefore, in this particular kernel it is essential to allocate A in FastMem, requiring very big amounts of available capacity, thus making the allocation of the rest data objects 
of total size 624 KB trivial. %\\

%\noindent
%\paragraph
{\bf 2mm:} Two matrix multiplications. Bandwith sensitive.\\
The kernel computation is $tmp = alpha*A*B$ and $D = tmp*C $. It uses an additional $tmp$ matrix to hold the intermediate multiplication of matrices $A$ and $B$. 
The partial ordering derived by the benefit values is $B = C > D = A =
B$. 
%ada: 
%\AG{what about temp, is that relevant?}
This is due to the fact that $B$ and $C$ receive significantly larger number of reads, even though 
all matrices are of the same size. When placing both B and C into FastMem, the benefit is 0.99, indicating that both data objects should be placed in fast. Since there is 
not significant benefit from placing the remaining data structures into FastMem, they can be allocated into SlowMem and reduce the space needed in FastMem more than 2 times.%\\

%\noindent
%\paragraph
{\bf covariance:} Calculation of covariance. Bandwith sensitive.\\
The kernel computes the covariance $cov_{x,y}=\frac{\sum_{i=1}^{N}(x_{i}-\bar{x})(y_{i}-\bar{y})}{N-1}$. 
The input resides in matrix $data$, the mean in matrix $mean$ and the result in matrix $symmat$.
The benefit values formulate an ordering of $data > symmat > mean$. The input matrix $data$ gets a very big amount of reads, compared to the other two matrices, 
thus incurs the biggest benefit value. In contrast, the result matrix $symmat$ even though has the same size as $data$, incurs only a few writes, leading to a trivial benefit value. 
Therefore, the only data structure that is essential to place in FastMem is the input matrix, the other two data structures can reside in SlowMem without impacting the overall 
application performance.%\\

%\noindent
%\paragraph
{\bf ludcmp:} LU decomposition. Bandwith sensitive.\\
The kernel represents a linear solver of $A*x = b$ using the LU decomposition of $A$ and transforming the problem into $A = L*U$ and $b = L*y$ and $y = U*x$, using 
matrices $A$, $x$, $y$ and $b$. The ordering of these matrices according to the benefit values is $A > y > x > b$. Similarly to previous kernels, the input matrix $A$ incurs 
much more reads and is bigger in size than the other matrices. Placing $A$ into FastMem is essential in order to get performance close to the one where all data resides in FastMem. \\

\noindent{\em Summary of observations. } In this section, we analyzed in depth 6 kernels of Polybench/C. We
were able to determine the benefit factor for each data structure,
which corresponds to the amount that each one 
contributes to performance, by observing the overall execution time when only that object was residing in FastMem. Also, we were able to validate that these 
benefit values correspond to cost values calculated in related work, taking into consideration access frequency, density and pattern parameters. 
In both ways, we are able to create a partial ordering of the different data objects of a kernel, that corresponds to the priority they should have in the order with which 
they are placed in FastMem, so as to increase application performance. Our observations can be summarized as follows:
\begin{tightenumerate}
\item In the case of non-sensitive kernels, there is no real benefit from data tiering. Data can reside in SlowMem, without impacting performance, and there is no need to keep track of access metrics. 
\item In the case of sensitive kernels, not all objects have high
  benefit factors. Instead there usually is a very noticeable difference into the benefit factor that classifies data objects into two distinct groups of 
really high benefit and really low benefit. In such cases, upon
identifying the low benefit objects there is no need to continuously
monitor their accesses, as they can be allocated in SlowMem, without
hurting performance. This can be leveraged in future dynamic hybrid
memory managers. 
\item In most cases, the data objects that hold the input data are the
  ones with highest benefit factor. This suggests that data flow or
  dependence analysis methods, part of future toolchains, can be
  leveraged to guide the data management across the memory substrate.
%\TD{complicated syntax in that sentence, don't quite understand}
\end{tightenumerate}
In conclusion, the above observations indicate the need to classify the data objects of an applications into the ones that significantly affect performance and the ones who don't. For the later ones, 
there is no need to keep track of their accesses and they can actually
leverage the presence of SlowMem, as they can reside there without
decreasing performance. This analysis 
%enables 
%us to debunk two very common 
%misconceptions. First, not all applications are sensitive to
%SlowMem. Second, not all but rather 
sheds insights that only 
a small subset of an application's data objects has significant impact
on the application's sensitivity to memory heterogeneity, and thus
only those should be considered for placement in FastMem. This can be
leveraged for developing lighter-weight solutions to dynamic data
tiering and data movement techniques. Furthermore, these high benefit
factor data structures, unfortunately, can have very a big size, 
and given the potentially limited FastMem capacity, this further
argues for development of dynamic methods. 
\end{comment}


